{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,X,y,proportion=0.8,shuffle=True, mini_batch=0):\n",
    "        \"\"\"\n",
    "        Dataset class provide tools to manage dataset\n",
    "\n",
    "        :param X: ndarray, features, highly recommand ndarray\n",
    "        :param y: ndarray, labels\n",
    "        :param proportion: number between 0 and 1, the proportion of train dataset and test dataset\n",
    "        :param shuffle: boolean,\n",
    "        :param mini_batch mini batch size, 0 by default, in this case no mini batch size dataset will be generated\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.trainset = None\n",
    "        self.testset = None\n",
    "        self.validationset = None\n",
    "        self.proportion = proportion\n",
    "        self.shuffle = shuffle\n",
    "        self.mini_batch = mini_batch\n",
    "        self.allset = np.concatenate((X,y),axis=1)\n",
    "        self.minisets = []\n",
    "\n",
    "        if self.shuffle:\n",
    "            # automatic distribution\n",
    "            self.distribute()\n",
    "\n",
    "    # @classmethod\n",
    "    # def imageset(cls, path, proportion = 0.8, shuffle = None):\n",
    "    #     pass\n",
    "\n",
    "    def distribute(self):\n",
    "        \"\"\"\n",
    "        This function will automatically distribute train and test dataset\n",
    "        call this function to reshuffle all the dataset and also generate new train and test set\n",
    "        \"\"\"\n",
    "        n = np.shape(self.X)[0]\n",
    "        samples = np.concatenate((self.X,self.y),axis=1)\n",
    "        random.shuffle(samples)\n",
    "        # sample train and test dataset\n",
    "        self.trainset = samples[0:round(n * self.proportion),:]\n",
    "        self.testset = samples[round(n * self.proportion) + 1:, :]\n",
    "\n",
    "    def getX(self):\n",
    "        return self.X\n",
    "\n",
    "    def gety(self):\n",
    "        return self.y\n",
    "\n",
    "    def getminibatch(self):\n",
    "        return self.mini_batch\n",
    "\n",
    "    def gettrainset(self):\n",
    "        \"\"\"\n",
    "        :return: return train dataset with respect of proportion\n",
    "        \"\"\"\n",
    "        return Dataset(self.trainset[:, 0:self.X.shape[1]], self.trainset[:, self.X.shape[1]:], mini_batch=self.mini_batch)\n",
    "\n",
    "    def gettestset(self):\n",
    "        \"\"\"\n",
    "        :return: test dataset with respect of proportion\n",
    "        \"\"\"\n",
    "        return Dataset(self.testset[:, 0:self.X.shape[1]], self.testset[:, self.X.shape[1]:], mini_batch=self.mini_batch)\n",
    "\n",
    "    def getminiset(self):\n",
    "        \"\"\"\n",
    "        get mini sets with mini batch size\n",
    "        :return: Dataset array\n",
    "        \"\"\"\n",
    "        spilit_list = np.arange(self.mini_batch, self.allset.shape[0], self.mini_batch)\n",
    "        minisets = np.split(self.allset, spilit_list)\n",
    "        for i in range(len(minisets)):\n",
    "            self.minisets.append(Dataset(minisets[i][:, 0:self.X.shape[1]], minisets[i][:, self.X.shape[1]:],shuffle =False, mini_batch=self.mini_batch))\n",
    "        return self.minisets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class NN:\n",
    "    import numpy as np\n",
    "    def __init__(self,dataset):\n",
    "        \"\"\"\n",
    "        This class contains Activation function util class, Layer class for construct networks, it contains several extend classes like LinearLayer, Conv2D etc.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        layer_list = [NN.Layer('Linear',3,10,'sigmoid',BN=True), NN.Layer('Linear',10,100,'sigmoid',BN=True),\n",
    "              NN.Layer('Linear',100,10,'sigmoid',BN=True),NN.Layer('Linear',10,3,'none') ]\n",
    "\n",
    "        dataset = Dataset(X, y, mini_batch= 64)\n",
    "\n",
    "        nn = NN(dataset)\n",
    "\n",
    "        layer_list is a list has 4 layers all are Layer class. Note that here we don't use LinearLayer,\n",
    "        to use LinearLayer, replace NN.Layer('Linear',3,10,'sigmoid',BN=True) as NN.LinearLayer(,3,10,'sigmoid',BN=True) or\n",
    "        NN.LinearLayer(3,10,'sigmoid'), NN.BN()\n",
    "\n",
    "        :param dataset: Dataset class\n",
    "        \"\"\"\n",
    "        # self.input = input\n",
    "        self.dataset = dataset\n",
    "        self.layer_list = []\n",
    "\n",
    "    def addlayers(self,layers):\n",
    "        self.layer_list = layers\n",
    "\n",
    "    def getlayers(self):\n",
    "        return self.layer_list\n",
    "\n",
    "    # activation functions\n",
    "    class ActivationFunc:\n",
    "        \"\"\"\n",
    "        ActivationFunc is an util class with different types of activation function.\n",
    "        it can\n",
    "        \"\"\"\n",
    "        @staticmethod\n",
    "        def sigmoid(x):\n",
    "            \"\"\"\n",
    "            Sigmoid function\n",
    "            \"\"\"\n",
    "            return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "        @staticmethod\n",
    "        def ReLU(x):\n",
    "            \"\"\"\n",
    "            :param x: ndarray,  \n",
    "            :return:\n",
    "            \"\"\"\n",
    "            return np.maximum(0, x)\n",
    "\n",
    "        @staticmethod\n",
    "        def LeakyReLU(x):\n",
    "            return  np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "        @staticmethod\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        @staticmethod\n",
    "        def none(x):\n",
    "            return x\n",
    "\n",
    "    # Layer class\n",
    "    class Layer:\n",
    "        def __init__(self, type, input_dim, output_dim, activation, BN = False):\n",
    "            \"\"\"\n",
    "            Define a layer contains activation function or other normalization.\n",
    "\n",
    "            :param type: Layer type, choose 'Linear', 'Conv' etc\n",
    "            :param input_dim: input dim or previous layer's output\n",
    "            :param output_dim: output dim of this layer\n",
    "            :param activation: activation function, it now support \"sigmoid\", \"ReLU\", \"LeakyReLU\", \"tanh\" and \"none\" for no activation function\n",
    "            :param BN, batch normalization , Default False\n",
    "\n",
    "            Examples:\n",
    "\n",
    "            A linear layer with input dim = 3 and output dim = 10, following batch normalization and a sigmoid activation function\n",
    "            NN.Layer('Linear',3,10,'sigmoid',BN=True)\n",
    "\n",
    "            \"\"\"\n",
    "            self.type = type\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "            self.activation = activation\n",
    "            self.BN = BN\n",
    "\n",
    "        def getinputdim(self):\n",
    "            return self.input_dim\n",
    "\n",
    "        def getoutputdim(self):\n",
    "            return self.output_dim\n",
    "\n",
    "        def gettype(self):\n",
    "            return self.type\n",
    "\n",
    "        def getact(self, x):\n",
    "            func_name = \"NN.ActivationFunc.\"+self.activation\n",
    "            func = eval(func_name)\n",
    "            return func(x)\n",
    "\n",
    "        def getactname(self):\n",
    "            return self.activation\n",
    "\n",
    "        def getBN(self):\n",
    "            return self.BN\n",
    "\n",
    "    class LinearLayer(Layer):\n",
    "        \"\"\"\n",
    "        Define a linear layer\n",
    "\n",
    "        As same as Layer except no need to clarify type\n",
    "        \"\"\"\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            self.type = \"Linear\"\n",
    "            self.input_dim = input_dim\n",
    "            self.output_dim = output_dim\n",
    "\n",
    "\n",
    "    class Conv2DLayer(Layer):\n",
    "        \"\"\"\n",
    "        Define a 2D convolutional layer_\n",
    "        \"\"\"\n",
    "        def __init__(self, input_size, kernel_size, stride, padding):\n",
    "            \"\"\"\n",
    "            initialize 2D conv layer\n",
    "\n",
    "            :param input_size: Union[tuple, ndarray]  layer's input size\n",
    "            :param kernel_size:  Union[tuple, ndarray] layer's kernel size\n",
    "            :param stride: Int\n",
    "            :param padding: Int\n",
    "            \"\"\"\n",
    "            self.type = \"Conv2D\"\n",
    "            self.input_size = input_size\n",
    "            self.kernel_size = kernel_size\n",
    "            self.stride = stride\n",
    "            self.padding = padding\n",
    "\n",
    "        def getimagesize(self):\n",
    "            return self.image_size\n",
    "\n",
    "        def getkernelsize(self):\n",
    "            return self.kernel_size\n",
    "\n",
    "        def getstride(self):\n",
    "            return self.stride\n",
    "\n",
    "        def getpadding(self):\n",
    "            return self.padding\n",
    "\n",
    "    class BN(Layer):\n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            Define a batch normalization layer\n",
    "            \"\"\"\n",
    "            self.type = \"BN\"\n",
    "            self.activation =\"none\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,nn ,optimizer,loss_function, batch_size=8,epoch=20000,lr=0.0001,decay_rate=0):\n",
    "        \"\"\"\n",
    "        :param nn: input an NN class\n",
    "        :param optimizer: optimizer as \"GD\", \"SGD\" etc\n",
    "        :param batch_size: batch size for mini batch optimization\n",
    "        :param epoch: epoch number\n",
    "        :param lr: learning rate\n",
    "        :param decay_rate: float, learning rate decay rate by default is 0\n",
    "        \"\"\"\n",
    "\n",
    "        self.nn = nn\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.weight_list = None\n",
    "        self.gradient_list = None\n",
    "        self.loss_list = None\n",
    "        self.passer_list = None\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def getgradientlist(self):\n",
    "        return self.gradient_list\n",
    "\n",
    "    def getlosslist(self):\n",
    "        return self.loss_list\n",
    "\n",
    "    def getweightlist(self):\n",
    "        return self.weight_list\n",
    "\n",
    "    class LossFunc:\n",
    "        class Logarithmic:\n",
    "            def __init__(self, y_true=None, y_pred=None, eps=1e-16):\n",
    "                self.y_true = y_true\n",
    "                self.y_pred = y_pred\n",
    "                self.eps = eps\n",
    "                \"\"\"\n",
    "                Loss function we would like to optimize (minimize)\n",
    "                We are using Logarithmic Loss\n",
    "                http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss\n",
    "                \"\"\"\n",
    "            def loss(self):\n",
    "                self.y_pred = np.maximum(self.y_pred, self.eps)\n",
    "                self.y_pred = np.minimum(self.y_pred, (1 - self.eps))\n",
    "                return -(np.sum(self.y_true * np.log(self.y_pred)) + np.sum((1 - self.y_true) * np.log(1 - self.y_pred))) / len(self.y_true)\n",
    "\n",
    "        class Quadratic:\n",
    "            def __init__(self, y_true=None, y_pred=None, norm = 0):\n",
    "                self.y_true = y_true\n",
    "                self.y_pred = y_pred\n",
    "                self.norm = norm\n",
    "\n",
    "            def loss(self):\n",
    "                return 1 / self.y_true.shape[0] * 0.5 * np.sum((self.y_pred - self.y_true) ** 2)\n",
    "\n",
    "            def diff(self):\n",
    "                return 2 * (self.y_pred - self.y_true)\n",
    "\n",
    "        class MSE:\n",
    "            def __init__(self, y_true=None, y_pred=None, x=None):\n",
    "                self.y_true = y_true\n",
    "                self.y_pred = y_pred\n",
    "                self.x = x\n",
    "\n",
    "            def loss(self):\n",
    "                return 1 / np.shape(self.y_true)[0] * np.sum((self.y_pred - self.y_true) ** 2)\n",
    "\n",
    "            def diff(self):\n",
    "                return 2 / np.shape(self.y_true)[0] * np.sum(self.x @ (self.y_pred - self.y_true))\n",
    "\n",
    "\n",
    "    class Node:\n",
    "        def __init__(self, data: np.ndarray, type : str):\n",
    "            \"\"\"\n",
    "            Node class, is the node of binary tree which has two child node: left and right.\n",
    "            It can also be presented as weight. Every passer during the back propagation is saved as\n",
    "            a node class contains data, type, back and cache for calculation\n",
    "\n",
    "            :param data: ndarray, value given during forward propagation\n",
    "            :param type: str, the type of node, it can be \"weight\", \"data\" or calculation like \"@\", \"+\" etc\n",
    "            :param back: ndarray, value updated during back propagation\n",
    "            :param cache: array_like stock forward propagation's detail and middle value for the convenient of back propagation\n",
    "            \"\"\"\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.data = data\n",
    "            self.type = type\n",
    "            self.back = None\n",
    "            self.cache = None\n",
    "            self.momentum = None\n",
    "\n",
    "        def getleft(self):\n",
    "            return self.left\n",
    "\n",
    "        def getright(self):\n",
    "            return self.right\n",
    "\n",
    "        def gettype(self):\n",
    "            return self.type\n",
    "\n",
    "        def getdata(self):\n",
    "            return  self.data\n",
    "\n",
    "        def getback(self):\n",
    "            return  self.back\n",
    "\n",
    "        def getmomentum(self):\n",
    "            return self.momentum\n",
    "\n",
    "    class WeightIni:\n",
    "        \"\"\"\n",
    "        Provide weight initial functions. util class\n",
    "        \"\"\"\n",
    "        @staticmethod\n",
    "        def init_linear_weight(input_dim, output_dim):\n",
    "            return np.random.uniform(-1, 1, (input_dim, output_dim))\n",
    "\n",
    "        @staticmethod\n",
    "        def init_BN_weight(dim):\n",
    "\n",
    "            return np.ones((1, dim)), np.ones((1, dim), dtype=\"float32\")\n",
    "\n",
    "        @staticmethod\n",
    "        def init_conv2D_kernel(shape):\n",
    "            \"\"\"\n",
    "            :param shape: Union[tuple, int, float] shape of kernel\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            return np.random.random(shape)\n",
    "\n",
    "        @staticmethod\n",
    "        def initial_weight_list(layer_list):\n",
    "            \"\"\"\n",
    "            @Staticmethod. Given layer list and return respected initiall weight list\n",
    "\n",
    "            :param layer_list: list, layer list\n",
    "            :return: list, list of weight in Node class\n",
    "            \"\"\"\n",
    "            weight_list = []\n",
    "            # initial weights in weight list by their type\n",
    "            layer_num = len(layer_list)\n",
    "            for i in range(layer_num):\n",
    "                # linear weight operation\n",
    "                if layer_list[i].gettype() == \"Linear\":\n",
    "                    weight_list.append(Optimizer.Node(Optimizer.WeightIni.init_linear_weight(layer_list[i].getinputdim(), layer_list[i].getoutputdim()),\"weight\"))\n",
    "                elif layer_list[i].gettype() == \"BN\":\n",
    "                    dim = layer_list[i-1].getoutputdim()\n",
    "                    gamma, beta = Optimizer.WeightIni.init_BN_weight(dim)\n",
    "                    weight_list.append(Optimizer.Node(gamma,\"weight\"))\n",
    "                    weight_list.append(Optimizer.Node(beta,\"weight\"))\n",
    "                    layer_list[i].input_dim = dim\n",
    "                    layer_list[i].output_dim = dim\n",
    "                # kernel parse operation\n",
    "                elif layer_list[i].gettype() == \"Conv2D\":\n",
    "                    weight_list.append(Optimizer.Node(Optimizer.WeightIni.init_conv2D_kernel(layer_list[i].getkernelsize()),\"weight\"))\n",
    "                else:\n",
    "                    return  NameError\n",
    "                # check if you need BN init\n",
    "                if layer_list[i].getBN():\n",
    "                    dim = layer_list[i].getoutputdim()\n",
    "                    gamma, beta = Optimizer.WeightIni.init_BN_weight(dim)\n",
    "                    weight_list.append(Optimizer.Node(gamma,\"weight\"))\n",
    "                    weight_list.append(Optimizer.Node(beta,\"weight\"))\n",
    "\n",
    "            return weight_list\n",
    "\n",
    "    @staticmethod\n",
    "    def forword(passer, weight_list, layer_list):\n",
    "        layer_num = len(layer_list)\n",
    "        passer_list = [Optimizer.Node(passer, \"data\")]\n",
    "        # Every layer not necessarily has only one weight, like BN has 2 weights in a single layer\n",
    "        weight_count = 0\n",
    "\n",
    "        for i in range(layer_num):\n",
    "            if layer_list[i].gettype() =='Linear':\n",
    "                passer = passer@weight_list[weight_count].getdata()\n",
    "                # append binary tree after inner product of weight and previous layer\n",
    "                node = Optimizer.Node(passer,\"@\")\n",
    "                node.left = passer_list[-1]\n",
    "                node.right = weight_list[weight_count]\n",
    "                passer_list.append(node)\n",
    "\n",
    "                weight_count += 1\n",
    "\n",
    "                if layer_list[i].getBN():\n",
    "                    node_cache = [passer, np.var(passer,axis = 0), np.mean(passer, axis=0 )]\n",
    "\n",
    "                    passer = (passer - np.mean(passer,axis=0))/np.sqrt(np.var(passer,axis=0))\n",
    "                    node = Optimizer.Node(passer,\"normalization\")\n",
    "                    node.cache = node_cache\n",
    "                    node.left = passer_list[-1]\n",
    "                    passer_list.append(node)\n",
    "\n",
    "                    node = Optimizer.Node(passer,\"*scalar\")\n",
    "                    node.left = passer_list[-1]\n",
    "                    node.right = weight_list[weight_count]\n",
    "                    passer_list.append(node)\n",
    "\n",
    "                    passer = passer + weight_list[weight_count+1].getdata()\n",
    "                    node = Optimizer.Node(passer,\"+scalar\")\n",
    "                    node.left = passer_list[-1]\n",
    "                    node.right = weight_list[weight_count+1]\n",
    "                    passer_list.append(node)\n",
    "\n",
    "                    weight_count += 2\n",
    "\n",
    "                passer = layer_list[i].getact(passer)\n",
    "                #append binary tree after activation function\n",
    "                node = Optimizer.Node(passer,layer_list[i].getactname())\n",
    "                node.left = passer_list[-1]\n",
    "                passer_list.append(node)\n",
    "\n",
    "            # elif layer_list[j].gettype() == \"Conv2D\":\n",
    "            else: raise NameError\n",
    "\n",
    "        return passer_list\n",
    "\n",
    "    @staticmethod\n",
    "    def backpropagation(node):\n",
    "        epsilon = 1e-8\n",
    "        if node.getleft() is not None:\n",
    "            if node.gettype() == \"@\":\n",
    "                node.getleft().back = node.getback()@node.getright().getdata().T\n",
    "                node.getright().back = node.getleft().getdata().T@node.getback()\n",
    "            elif node.gettype() == \"sigmoid\":\n",
    "                node.getleft().back = np.multiply(node.getback(),np.multiply(NN.ActivationFunc.sigmoid(node.getback()),\n",
    "                                                                             1-NN.ActivationFunc.sigmoid(node.getback())))\n",
    "            elif node.gettype() == \"ReLU\":\n",
    "                back = copy.deepcopy(node.getback())\n",
    "                back[back<=0] = 0\n",
    "                node.getleft().back = back\n",
    "            elif node.gettype() == \"LeakyReLU\":\n",
    "                back = copy.deepcopy(node.getback())\n",
    "                back[back<0] = 0.01*back[back<0]\n",
    "                node.getleft().back = back\n",
    "            elif node.gettype() == \"tanh\":\n",
    "                node.getleft().back = np.multiply((np.ones(node.getback().shape)-NN.ActivationFunc.tanh(node.getback())**2),\n",
    "                                                  node.getback())\n",
    "            elif node.gettype() == \"+\":\n",
    "                node.getleft().back = node.getback()\n",
    "                node.getright().back = node.getback()\n",
    "            elif node.gettype() == \"-\":\n",
    "                node.getleft().back = node.getback()\n",
    "                node.getright().back = -node.getback()\n",
    "            elif node.gettype() == \"+scalar\":\n",
    "                node.getleft().back = node.getback()\n",
    "                node.getright().back = np.sum(node.getback(),axis=0)\n",
    "            elif node.gettype() == \"*scalar\":\n",
    "                node.getleft().back = node.getright().getdata() * node.getback()\n",
    "                node.getright().back = np.sum(node.getleft().getdata().T,axis=0)@node.getback()\n",
    "            elif node.gettype() == \"none\":\n",
    "                node.getleft().back = node.getback()\n",
    "            elif node.gettype() == \"normalization\":\n",
    "                # cache = [x,  sigma_beta^2, mu_beta]\n",
    "\n",
    "                # dx = 1/N / std * (N * dx_norm -\n",
    "                #       dx_norm.sum(axis=0) -\n",
    "                #       x_norm * (dx_norm * x_norm).sum(axis=0))\n",
    "\n",
    "                x = node.cache[0]\n",
    "                sigma2 = node.cache[1]\n",
    "                mu = node.cache[2]\n",
    "\n",
    "                dl_dx_hat = node.getback()\n",
    "                dl_dsigma2 = np.sum(dl_dx_hat,axis=0) * (x-mu) * -0.5*(sigma2+epsilon)**-3/2\n",
    "                dl_dmu = np.sum(dl_dx_hat,axis=0) * -1/np.sqrt(sigma2+epsilon) + dl_dsigma2 * np.sum(-2*(x-mu),axis= 0)/x.shape[0]\n",
    "                dl_dx = dl_dx_hat * 1/np.sqrt(sigma2+epsilon) + dl_dsigma2*2*(x-mu)/x.shape[0] + dl_dmu /x.shape[0]\n",
    "                node.getleft().back = dl_dx\n",
    "\n",
    "            Optimizer.backpropagation(node.getleft())\n",
    "        else:\n",
    "            return\n",
    "\n",
    "    def lrdecay(self, iter):\n",
    "        \"\"\"\n",
    "        Learning rate decay function. Given iteration, modify learning rate\n",
    "\n",
    "        :param iter: int, iteration count\n",
    "        \"\"\"\n",
    "        self.lr =  1 / (1 + self.decay_rate * iter) * self.lr\n",
    "\n",
    "    def GD(self, root: Node, weight_list):\n",
    "        \"\"\"\n",
    "        Gradient descent, do the back propagation and update weight list\n",
    "\n",
    "        :param root: Node, the root of passer binary tree\n",
    "        :param weight_list: list, weight list\n",
    "        :return: list, updated weight list\n",
    "        \"\"\"\n",
    "        Optimizer.backpropagation(root)\n",
    "        gradient_list = []\n",
    "\n",
    "        for node in weight_list:\n",
    "            node.data = node.data - self.lr * node.back\n",
    "            gradient_list.append(node.back)\n",
    "        return weight_list, gradient_list\n",
    "\n",
    "    def SGD(self, weight_list, passer_list):\n",
    "        # we resume mini-batch equals 1 each time\n",
    "        \"\"\"\n",
    "        Stochastic gradient descent. It takes weight list and passer list as inputs, it will\n",
    "        :param weight_list:\n",
    "        :param passer_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        def init_random_node(node, random_num_list, mini_weight_list):\n",
    "            node.data = node.data[random_num_list,:]\n",
    "            node.back = None\n",
    "            if node.getright() is not None:\n",
    "                mini_weight_list.append(node.getright())\n",
    "            if node.getleft() is not None:\n",
    "                init_random_node(node.getleft(), random_num_list, mini_weight_list)\n",
    "            else: return\n",
    "\n",
    "        # obs = observation number = output layer's dim 0\n",
    "        num_obs = self.nn.dataset.gettrainset().getX().shape[0]\n",
    "        mini_passer_list = copy.deepcopy(passer_list)\n",
    "        root = mini_passer_list[-1]\n",
    "        gradient_list = []\n",
    "\n",
    "        # randomly pick observations from original obs\n",
    "        random_num_list = np.random.randint(0, num_obs, num_obs)\n",
    "\n",
    "        # initial random node\n",
    "        mini_weight_list = []\n",
    "        init_random_node(root, random_num_list, mini_weight_list)\n",
    "\n",
    "        # back propagation\n",
    "        root.back = 2 * (- self.nn.dataset.gettrainset().gety()[random_num_list] + root.getdata()[random_num_list])\n",
    "        Optimizer.backpropagation(root)\n",
    "\n",
    "        i = 0\n",
    "        # update weight list\n",
    "        for weight in weight_list:\n",
    "            weight.data = weight.data - self.lr * mini_weight_list[-i-1].back\n",
    "            gradient_list.append(mini_weight_list[-i-1].back)\n",
    "            i = i + 1\n",
    "\n",
    "        return weight_list, gradient_list\n",
    "\n",
    "    def momentumgd(self, root: Node, weight_list, beta = 0.2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param root: Node, the root of passer binary tree\n",
    "        :param weight_list: list, weight list\n",
    "        :param beta: momentum conservation rate\n",
    "        :return: list, updated weight list\n",
    "        \"\"\"\n",
    "        Optimizer.backpropagation(root)\n",
    "        gradient_list = []\n",
    "\n",
    "        for node in weight_list:\n",
    "            if node.getmomentum() is None:\n",
    "                node.momentum = (1 - beta) * node.getback()\n",
    "            else:\n",
    "                node.momentum = beta * node.getmomentum() + (1 - beta) * node.getback()\n",
    "            node.data = node.getdata() - self.lr * (1 - beta) * node.getback()\n",
    "            gradient_list.append(node.back)\n",
    "        return weight_list, gradient_list\n",
    "\n",
    "    def RMSprop(self, root: Node, weight_list, beta = 0.2, eps =1e-10):\n",
    "\n",
    "        Optimizer.backpropagation(root)\n",
    "        gradient_list = []\n",
    "\n",
    "        for node in weight_list:\n",
    "            if node.getmomentum() is None:\n",
    "                node.momentum = (1 - beta) * node.getback() ** 2\n",
    "            else:\n",
    "                node.momentum = beta * node.getmomentum() + (1 - beta) * node.getback() ** 2\n",
    "\n",
    "            node.data = node.getdata() - self.lr * node.getback() / (np.sqrt(node.getmomentum()) + eps)\n",
    "            gradient_list.append(node.back)\n",
    "        return weight_list, gradient_list\n",
    "\n",
    "\n",
    "    def Adam(self, root: Node, weight_list, beta_mom = 0.2, beta_rms = 0.2, eps = 1e-10):\n",
    "        \"\"\"\n",
    "        Adam optimizer\n",
    "        :param root:\n",
    "        :param weight_list:\n",
    "        :param beta_mom:\n",
    "        :param beta_rms:\n",
    "        :param eps:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        Optimizer.backpropagation(root)\n",
    "        gradient_list = []\n",
    "\n",
    "        for node in weight_list:\n",
    "            if node.getmomentum() is None:\n",
    "                node.momentum = [(1 - beta_mom) * node.getback(), (1 - beta_rms) * node.getback() ** 2]\n",
    "            else:\n",
    "                node.momentum[0] = (beta_mom * node.getmomentum()[0] + (1 - beta_mom) * node.getback()) / (1 - beta_mom)\n",
    "                node.momentum[1] = (beta_rms * node.getmomentum()[1] + (1 - beta_rms) * node.getback() ** 2 ) / (1 - beta_rms)\n",
    "\n",
    "            node.data = node.getdata() - self.lr * node.getmomentum()[0] / (np.sqrt(node.getmomentum()[1])+eps)\n",
    "            gradient_list.append(node.back)\n",
    "        return weight_list, gradient_list\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        train process, it will first initial weight, loss, gradient and passer list, then, optimize weights by given optimizer.\n",
    "        In the end, calculate loss and step to the next epoch.\n",
    "\n",
    "        It will finally stock all the weight, loss, gradient and passer during the training process\n",
    "        \"\"\"\n",
    "        layer_list = self.nn.getlayers()\n",
    "\n",
    "        # initial weight, loss and gradient list\n",
    "        self.weight_list = [[] for i in range(self.epoch+1)]\n",
    "        self.weight_list[0] = Optimizer.WeightIni.initial_weight_list(layer_list)\n",
    "        self.loss_list = np.zeros(self.epoch)\n",
    "        self.gradient_list = [[] for i in range(self.epoch)]\n",
    "        self.passer_list = [[] for i in range(self.epoch)]\n",
    "\n",
    "        # for GD and SGD, they use full dataset, so need only read X and y once\n",
    "        if self.optimizer ==\"GD\" or self.optimizer == \"SGD\":\n",
    "            X = self.nn.dataset.gettrainset().getX()\n",
    "            X = Optimizer.Node(X, \"data\")\n",
    "            for i in range(self.epoch):\n",
    "                # forward propagation\n",
    "                self.passer_list[i] = Optimizer.forword(X.getdata(), self.weight_list[i],layer_list)\n",
    "                root = self.passer_list[i][-1]\n",
    "\n",
    "                # calculate loss by using: loss 2 * (-self.nn.dataset.gettrainset().gety() + root.getdata())\n",
    "                loss_func = self.loss_function(self.nn.dataset.gettrainset().gety(), root.getdata())\n",
    "                self.loss_list[i] = loss_func.loss()\n",
    "\n",
    "                root.back = loss_func.diff()\n",
    "                # upgrade gradient by selected optimizer\n",
    "                if self.optimizer ==\"GD\":\n",
    "                    self.weight_list[i+1], self.gradient_list[i] = Optimizer.GD(self, root, self.weight_list[i])\n",
    "\n",
    "                elif self.optimizer ==\"SGD\":\n",
    "                    self.weight_list[i+1], self.gradient_list[i]  = Optimizer.SGD(self, self.weight_list[i], self.passer_list[i])\n",
    "\n",
    "        # mini batch type gradient descent\n",
    "        else:\n",
    "            for i in range(self.epoch):\n",
    "                start_time = time.time()\n",
    "                # get mini batch\n",
    "                minisets = self.nn.dataset.gettrainset().getminiset()\n",
    "                epoch_weight_list = [copy.deepcopy(self.weight_list[i])]\n",
    "                epoch_loss_list = np.zeros(len(minisets))\n",
    "\n",
    "                # GD for every mini batch\n",
    "                for j in range(len(minisets)):\n",
    "                    X_bar = minisets[j]\n",
    "                    self.passer_list[i].append(Optimizer.forword(X_bar.getX(), epoch_weight_list[j], layer_list))\n",
    "\n",
    "                    root = self.passer_list[i][j][-1]\n",
    "                    loss_func = self.loss_function(X_bar.gety(), root.getdata())\n",
    "\n",
    "                    epoch_loss_list[j] = loss_func.loss()\n",
    "                    root.back = loss_func.diff()\n",
    "                    root.momentum = root.getback()\n",
    "\n",
    "                    if self.optimizer == \"minibatchgd\":\n",
    "                        weight, gradient = Optimizer.GD(self, root, epoch_weight_list[j])\n",
    "                    elif self.optimizer == \"momentumgd\":\n",
    "                        weight, gradient = Optimizer.momentumgd(self, root, epoch_weight_list[j])\n",
    "                    elif self.optimizer == \"RMSprop\":\n",
    "                        weight, gradient = Optimizer.RMSprop(self, root, epoch_weight_list[j])\n",
    "                    elif self.optimizer == \"Adam\":\n",
    "                        weight, gradient = Optimizer.Adam(self, root, epoch_weight_list[j])\n",
    "                    else: raise NameError\n",
    "                    epoch_weight_list.append(weight)\n",
    "\n",
    "                self.weight_list[i+1]= epoch_weight_list[-1]\n",
    "                self.gradient_list[i] = gradient\n",
    "\n",
    "                self.loss_list[i] = sum(epoch_loss_list)/len(epoch_loss_list)\n",
    "\n",
    "                # learnign rate decay\n",
    "\n",
    "                self.lrdecay(i)\n",
    "                # every epoch shuffle the dataset\n",
    "                self.nn.dataset.distribute()\n",
    "\n",
    "                if (i + 1) % 1  ==0:\n",
    "                    used_time = time.time() - start_time\n",
    "                    print(\"epoch \" + str(i + 1) + ', Training time: %.4f' % used_time + ', Training loss: %.6f' % self.loss_list[i])\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Use trained weight on testset for the evaluation of the model\n",
    "        :return: model prediction and loss on the testset\n",
    "        \"\"\"\n",
    "        weight = self.weight_list[-1]\n",
    "        layer_list = self.nn.getlayers()\n",
    "        testset = self.nn.dataset.gettestset()\n",
    "        passer = testset.getX()\n",
    "\n",
    "        passer_list = self.forword(passer,weight,layer_list)\n",
    "        predicted = passer_list[-1].getdata()\n",
    "\n",
    "        loss = self.loss_function.loss(testset.gety(), predicted)\n",
    "        return predicted, loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use trained weight on X and output prediction\n",
    "        :param X: ndarray, feature data wish to be predicted\n",
    "        :return: model's prediction by using trained data\n",
    "        \"\"\"\n",
    "        passer = X\n",
    "        weight = self.weight_list[-1]\n",
    "        passer_list = self.forword(passer, weight, self.nn.getlayers())\n",
    "        return passer_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Visual:\n",
    "    def __init__(self, optim):\n",
    "        self.optim = optim\n",
    "\n",
    "    def plotloss(self):\n",
    "        \"\"\"\n",
    "        :return: plot loss flow during the training\n",
    "        \"\"\"\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "        fig = plt.figure()\n",
    "        ax = plt.axes()\n",
    "        ax.plot(self.optim.loss_list, label = 'loss')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_ylabel('Loss during the training')\n",
    "\n",
    "    def plotgradientnorm(self):\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "        fig, axs = plt.subplots(len(self.optim.getgradientlist()[0]))\n",
    "        for i in range(len(self.optim.getgradientlist()[0])):\n",
    "            gradient_norm_list = []\n",
    "            for j in range(len(self.optim.getgradientlist())):\n",
    "                gradient_norm_list.append(np.linalg.norm(self.optim.getgradientlist()[j][i]))\n",
    "            axs[i].plot(gradient_norm_list, label = 'norm 2')\n",
    "            axs[i].legend(loc='upper right')\n",
    "            axs[i].set_ylabel('W' + str(i) +\" norm\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# total observation number\n",
    "n = 300\n",
    "# x1, x2 are generated by two\n",
    "x1 = np.random.uniform(0,1,n)\n",
    "x2 = np.random.uniform(0,1,n)\n",
    "const = np.ones(n)\n",
    "eps = np.random.normal(0,.05,n)\n",
    "b = 1.5\n",
    "theta1 = 2\n",
    "theta2 = 5\n",
    "Theta = np.array([b, theta1, theta2])\n",
    "y = np.array(b * const+ theta1 * x1 + theta2 * x2 + eps)\n",
    "y=np.reshape(y,(-1,1))\n",
    "X = np.array([const,x1,x2]).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_15763/4253886332.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mloss_func\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLossFunc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mQuadratic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0moptim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOptimizer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\"SGD\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mloss_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10000\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1e-6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mvisual\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mVisual\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mvisual\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplotloss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m                 \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptimizer\u001B[0m \u001B[0;34m==\u001B[0m\u001B[0;34m\"SGD\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 438\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSGD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpasser_list\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    439\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    440\u001B[0m         \u001B[0;31m# mini batch type gradient descent\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mSGD\u001B[0;34m(self, weight_list, passer_list)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \u001B[0;31m# back propagation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m         \u001B[0mroot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgettrainset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgety\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrandom_num_list\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mroot\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetdata\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrandom_num_list\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackpropagation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m         \u001B[0mi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mbackpropagation\u001B[0;34m(node)\u001B[0m\n\u001B[1;32m    270\u001B[0m                 \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdl_dx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m             \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackpropagation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mbackpropagation\u001B[0;34m(node)\u001B[0m\n\u001B[1;32m    270\u001B[0m                 \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdl_dx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m             \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackpropagation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mbackpropagation\u001B[0;34m(node)\u001B[0m\n\u001B[1;32m    270\u001B[0m                 \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdl_dx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m             \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackpropagation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mbackpropagation\u001B[0;34m(node)\u001B[0m\n\u001B[1;32m    270\u001B[0m                 \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdl_dx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 272\u001B[0;31m             \u001B[0mOptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackpropagation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    273\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_15763/766680957.py\u001B[0m in \u001B[0;36mbackpropagation\u001B[0;34m(node)\u001B[0m\n\u001B[1;32m    234\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgettype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"LeakyReLU\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    235\u001B[0m                 \u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeepcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 236\u001B[0;31m                 \u001B[0mback\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mback\u001B[0m\u001B[0;34m<\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.01\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mback\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mback\u001B[0m\u001B[0;34m<\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    237\u001B[0m                 \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetleft\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mback\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    238\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0mnode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgettype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"tanh\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "layer_list = [NN.Layer('Linear',3,100,'LeakyReLU'),NN.Layer('Linear',100,3,'LeakyReLU'),\n",
    "              NN.Layer('Linear',3,1,'none')]\n",
    "dataset = Dataset(X, y)\n",
    "nn = NN(dataset)\n",
    "nn.addlayers(layer_list)\n",
    "loss_func = Optimizer.LossFunc.Quadratic\n",
    "optim = Optimizer(nn,\"SGD\",loss_func, epoch = 10000, lr=1e-6)\n",
    "optim.train()\n",
    "visual = Visual(optim)\n",
    "visual.plotloss()\n",
    "visual.plotgradientnorm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# total observation number\n",
    "n = 10000\n",
    "# x1, x2 are generated by two\n",
    "x1 = np.random.uniform(0,1,n)\n",
    "x2 = np.random.uniform(0,1,n)\n",
    "const = np.ones(n)\n",
    "eps = np.random.normal(0,.05,n)\n",
    "b = 1.5\n",
    "theta1 = 2\n",
    "theta2 = 5\n",
    "Theta = np.array([b, theta1, theta2])\n",
    "y = np.array(b * const+ theta1 * x1 + theta2 * x2 + eps)\n",
    "y=np.reshape(y,(-1,1))\n",
    "X = np.array([const,x1,x2]).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_list = [NN.Layer('Linear',3,10,'sigmoid',BN=True), NN.Layer('Linear',10,100,'sigmoid',BN=True),\n",
    "              NN.Layer('Linear',100,10,'sigmoid',BN=True),NN.Layer('Linear',10,3,'none') ]\n",
    "dataset = Dataset(X, y, mini_batch= 64)\n",
    "nn = NN(dataset)\n",
    "nn.addlayers(layer_list)\n",
    "loss_func = Optimizer.LossFunc.Quadratic\n",
    "optim = Optimizer(nn,\"Adam\", loss_func, epoch = 10, lr=1e-2, decay_rate=0.01)\n",
    "optim.train()\n",
    "visual = Visual(optim)\n",
    "visual.plotloss()\n",
    "visual.plotgradientnorm()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}